\documentclass[reprint,superscriptaddress,prb,showkeys]{revtex4-2}
\usepackage{empheq}
\usepackage{float}
\usepackage{lipsum}
\usepackage{mathtools, cuted}
\usepackage{esvect}
\usepackage{xcolor}
\usepackage[caption = false]{subfig}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{physics}
\usepackage[mathlines]{lineno}
\usepackage[capitalise]{cleveref}
\usepackage[english]{babel}
\usepackage{orcidlink}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{listings}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    columns=fullflexible,
    frame=single,
    breaklines=true,
    captionpos=b,
    numbers=left,
    numberstyle=\tiny\color{gray},
    escapeinside={(*@}{@*)}
}
\usepackage{algorithm}
\usepackage{algpseudocode}

%------------- COMMANDS -------------

\newcommand{\C}{\mathbb{C}} %complex
\newcommand{\R}{\mathbb{R}} %reals
\newcommand{\Z}{\mathbb{Z}} %integers
\renewcommand{\P}{\mathbb{P}} %prob

\newcommand{\m}{\text{m}} %m index
\newcommand{\M}{\text{M}} %m index
\newcommand{\N}{\text{N}} %n index
\newcommand{\n}{\text{n}} %n index
\renewcommand{\a}{\textbf{\text{a}}} %a index

\newcommand{\brac}[1]{\left(#1 \right)} %nice brackets
\newcommand{\bk}[1]{\langle #1 \rangle} 

\newcommand{\TODO}[1]{\textcolor{red}{TODO: #1}}


%------------- MAIN -------------

\begin{document}

\title{Efficient Machine Learning in a Memristor Network}

\author{Monica Conte}
\affiliation{Soft Condensed Matter \& Biophysics, Debye Institute for Nanomaterials Science, Utrecht University, Princetonplein 1, 3584 CC Utrecht, The Netherlands.}

\date{\today}

\begin{abstract}
Inspired by the brain's energy-efficient processing, we propose a physical implementation of artificial neural networks to surpass digital systems in speed and energy efficiency. We consider a network with nodes connected by conical microfluidic memristor channels filled with aqueous electrolyte [1]. These memristors exhibit tunable conductance, responsive to electrical, chemical, and mechanical stimuli, as well as channel geometry. Based on this adaptability, we develop a hybrid training algorithm that adjusts either channel geometry or external stimuli as weights. Physical relaxation replaces the feedforward pass, while a combined physical-numerical gradient descent drives the backward pass. This approach enables tasks such as input-output voltage mapping and linear regression.
\end{abstract}
\maketitle

\renewcommand{\figurename}{FIG.}

\section{\label{sec:Intro}Introduction}

% ## Artificial Neural Networks ##
% - What they can achieve
% - Energy cost
% - Explain the two phases

Modern computing algortihms are able to efficiently perform complicated nuemrical and symbolic operations, but their sequential structure limits their performance on more perceptual tasks. Inspired by the architecture and the functioning of the human brain that performs tasks via distributed and parallel fashion, artificial neural network (ANN) algorithms outreached traditional computing achieving remarkable success in challenging tasks ranging from image recognition to natural language processing and became central to many technological advancements in the field of artificial intelligence.

Recent advances in AI systems have been powered by the silicon-based computing power of the Graphics Processing Units (GPUs) and by an unprecedented abundance of data. With a clear trend of larger ANN models, their the energy requirements have grown significantly, negatively impacting their efficiency and scalability\cite{energy_cost_ml1, energy_cost_ml2}. With the rapid increase in computation requirements of AI training and the slower improvemnts in computing hardware traditionally predicted by Moore's Law, the necessity of new computing paradigms became more relevant.  As an alternative to ANN performed strictly in-silico, physical neural networks (PNN) have been introduced, where physical processes rather than mathematical operations are trained\cite{trainingphysicalneuralnetworks}. Since physical processes can be faster and more energetically efficient than their digital representation, this direction constitutes a promosing research direction to address this issue.

Applications of PNNs have been explored across diverse fields, including optics \cite{optics1}, electronics \cite{electronics1}, and fluidic and mechanical systems \cite{}, among others. Many existing training algorithms rely entirely on in silico methods, which often face challenges when transferring the learned model to real physical devices. To address this, hybrid training approaches have been proposed, where the inference phase is executed physically while the backward phase is computed numerically. Other methods, based on coupled learning, implement local learning rules at the level of each connection or edge. However, these approaches still face the need for two identical physical systems to apply different boundary conditions. In this work, we propose a novel algorithm in which both inference and backpropagation are carried out through physical processes, specifically leveraging the natural relaxation dynamics of the system in response to external stimuli and weight modifications.

A key advantage of PNNs is their ability to more closely emulate certain functional aspects of biological neural systems. One of the defining features of the brain is its capacity to process and integrate multiple types of stimuli—such as electrical, chemical, and mechanical inputs—simultaneously. This multimodal responsiveness is believed to contribute significantly to the brain’s efficiency and adaptability \cite{}. Inspired by this, we focus on neural networks constructed from memristive elements. Specifically, we utilize ionic microfluidic channels in aqueous solutions\cite{Rene_IonicNeuromorphic}, which exhibit the unique ability to respond concurrently to diverse physical stimuli. This characteristic is central to the operation of our proposed training algorithm.

% ## What we could achive ##

% ## Structure of the paper ##

The paper is organized as follows. Section 2 reviews related work on memristor networks, machine learning algorithms, and physical learning methods. Section 3 describes the methods used in our study, including the network model, machine learning algorithm, and the physical learning approach. In Section 4, we present our proposed method for coupling memristors with capacitors and discuss the necessary properties for effective training. Section 5 details our experimental setup and implementation. The results and analysis are provided in Section 6, followed by conclusions and suggestions for future work in Section 7.

\newpage

\section{\label{sec:memristor_network}Memristor Network}

The PNN we consider is an electrical network made of nodes connected by resistive elements. As resistive elements, we consider conical microfluidic channels, schematically represented in \cref{fig:memristor_scheme}. 
\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{plots/memristor/memristor_scheme.pdf}
    \caption{A schematic representation of the conical fluidic memristor. A cone of lenght $L$, base radius $R_b$ and tip radius $R_t$ with charged surface connnects two bulk reservoirs of a 1:1 electrolyte in an acqueous solution. The bulk density in the base $\rho_b$ differs from the bulk density in the tip $\rho_t$. At the far side of the memristor, an electrical potential and pressure drop are applied. Overall, this results in a chemical, electrical and mechanical sensistive system.}\label{fig:memristor_scheme}
\end{figure} 
An azimuthally symmetric conical channel connects two reservoirs that contain an incompressible aqueous 1:1 electrolyte with viscosity $\eta=1.01mPa s$ containing ions with diffusion coefficients $D = 1.75\mu m^2 ms^{-1}$ and charge $\pm e$, with $e$ the proton charge. The cone has a base radius of $R_b = 200$nm, a tip radius of $R_t=50$nm and has length $L = 10\mu$m, which sets it in the long channel limit, where entrance and exit effects can be neglected. The walls of the channel are homogeneously charged and generate a surface potential $\psi_0$ that attracts the opposite-sign ions present in the solution due to the dissociation of the electrolyte. The electric potential decreases exponentially away from the surface due to the electrostatic screening of the attracted ions: an electric double layer (EDL) forms on the charged surfaces. 

At the far side of both reservoirs, electrical potential, pressure and ion concentration are imposed. As a result, the memristor feels a potential drop $\Delta V = V_b - V_t$ defined as the difference between the potential in the base and in the tip. The ions within the EDL region generate an electro-osmotic fluid flow $Q_V = (\pi R_b R_t\epsilon\psi_0/L\eta)\Delta V$. Meanwhile, the pressure drop $\Delta P = P_b - P_t$, given by the difference between the base and the tip pressure, induces a Poiseuille-like flow $Q_P = (\pi R_b^3 R_t^3/8L\eta\langle R^2\rangle)\Delta P$. Finally, the ions experience a concentration gradient $\Delta \rho = \rho_b - \rho_t$, defined by the difference between the ion concentration in the reservoir connected to the base of the channel, $\rho_b$, and the concentration in the reservoir connected to the tip, $\rho_t$. Consequently, a diffusive fluid flow arises. However, we consider only the pressure and potential contributions to the total fluid flow, $Q = Q_P + Q_V$, assuming the diffusive flow to be negligible.  The resulting fluid flow generates non-trivial ion concentration profiles which, in turn, directly infulence the channel's conductance $g$. 
%It has already been shown that, for $\Delta P = 0$, the channel shows current rectification, thus working as a diode \cite{}, due to concentration polarization.  

In the steady-state, the channel is exposed to constant chemical, pressure and electrical signal, the corresponding conductance $g_{\infty}\brac{\boldsymbol{\Delta}}$ depends on the three stimuli $\boldsymbol{\Delta}= (\Delta V, \Delta P, \Delta \rho)$ and is determined by the ion concentration profiles according to 
\[
\frac{g_{\infty}\brac{\boldsymbol{\Delta}}}{g_0} = \int_0^{L} \frac{\bar{\rho}\left(x, \boldsymbol{\Delta}\right)}{2\rho_b L} dx                                           
\]
where $g_0$ is the Ohmic conductance and $\bar{\rho}(x)$ the radially averaged salt concentration as a function of $x\in[0,L]$\cite{Rene_IonicNeuromorphic}. By solving the stationary condition of the total salt flux, an expression for the average salt concentration can be found and reads
\begin{multline}
\frac{\bar{\rho}\left(x, \boldsymbol{\Delta}\right)}{2\rho_b} = 1 - \frac{\Delta \rho}{\rho_b} \mathcal{I}\left(x, \Delta V, \Delta P\right) \\
 + \frac{ \rho_{in}\left( \Delta V \right) }{2\rho_b \text{Pe}} \left[ \mathcal{F}(x) - \mathcal{I}\left(x, \Delta V, \Delta P\right) \right]
\label{eq:rho_average}
\end{multline}
where we defined, for the sake of simplicity and readability, two functions
\[
\mathcal{I}\left(x, \Delta V, \Delta P\right) = \frac{e^{\frac{x}{L}\frac{R_t^2}{R_b R(x)} \text{Pe}}-1}{e^{\frac{R_t}{R_b} \text{Pe}}-1}, \ \ \ \ \mathcal{F}(x) = \frac{x}{L}\frac{R_t}{R(x)}.
\]
Moreover, we introduced an expression for the concentration inhomogeneity 
\[
\rho_{in}\brac{\Delta V} = 2\frac{(R_b-R_t)\sigma}{R_t^2}\frac{e\Delta V}{k_B T}
\]
and the Péclet number
$
\text{Pe} = QL/D\pi R_t^2
$
which quantifies the importance of the flow $Q$ over diffusion \cite{Rene_PressureSensitive}.

%To better deal with numerical instabilities, we use a limit expression of \cref{eq:rho_average} when $\text{Pe} \ll \left( R_b/R_t \right)^2$ \cite{Rene_PressureSensitive}. With this consideration, the expression for the average salt concentration we use is the following
%\[
%\frac{\rho(x)}{2\rho_b} = 
%\begin{array}{cc}
 % \{ & 
  %  \begin{array}{cc}
   %   0 & x\leq 0 \\
    %  \frac{100-x}{100} & 0\leq x\leq 100
    %\end{array}
%\end{array}
%\]

When the external stimuli vary dynamically in time, they generate ion accumulation and deplition in the channel, changing consequently the conductivity. We assume the dynamics of the time-dependent conductance $g(t)$ as a single exponential decay towards the steady-state conductance $g_{\infty}$ with a diffusion-like timescale $\tau = L^2/12D$, as in Ref. \cite{Rene_IonicNeuromorphic}. The equation for the conductance reads
\begin{equation}
\frac{\partial g\brac{\boldsymbol{\Delta}(t), t}}{\partial t} = \frac{g_{\infty}\brac{\boldsymbol{\Delta}(t)}-g\brac{\boldsymbol{\Delta}(t), t}}{\tau}
\label{eq:eqofmotconductance}
\end{equation}
with $\boldsymbol{\Delta}(t) = \brac{\Delta V\brac{t},\Delta P\brac{t}, \Delta \rho\brac{t}}$.
As in Ref. \cite{Rene_IonicNeuromorphic}, we consider the time scale relative to voltage-driven ion accumulation or depletion in the channel. We justify this choice by stating that pressure-driven dynamics is faster and recalling that we neglect chemically-driven transport. Due to the fact that the conductance depends on the history of the signals applied, this system is called memristor. Aside from the direct dependence of the conductance on external stimuli, there is also an implicit dependence on geometrical properties, such as length $L$ and base radius $R_b$, that we address in this work.

The PNN is composed of $\N$ nodes, an example is shown in \cref{fig:memristor_network}. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{plots/memristor/memristor_network.pdf}
    \caption{An example of a network of five nodes connected by six memristors. The nodes are divided in input nodes (green circles), hidden nodes (gray nodes) and output nodes (purple circles) each of them is characterized by a triplet of physical quantities, representing the potential, pressure, and density. Th resulting stimuli on the memristor is given by the difference between base and tip signal, it thus depends on the oreintation of the memristor. In the case of the memristor that connects the first and the second node, for example, the exteranl stimuli are $\Delta V = V_2 - V_1$, $\Delta P = P_2 - P_1$ and $\Delta \rho = \rho_2 - \rho_1$. The zig-zag lines directed towards the input nodes indicate where the electrical signal is applied, while the zig-zag lines pointing outwards from the nodes indicate where the electrical signal is measured.}
    \label{fig:memristor_network}
\end{figure} 
Each node $\n\in[1,\N]$ is characterized by a value of electrical potential $V_{\n}$, pressure $P_{\n}$ and bulk density $\rho_{\n}$. Even though each node in the network is characterized by three physical properties, only electrical current flows through the network. This current propagates via the memristive elements, enabling modifications to the electric potential at other nodes. In contrast, pressure and density are externally imposed at each node and do not propagate through the network. The set of nodes is divided in thee sets. A node belongs to the set of \textit{input nodes} if it is used to imput current in the circuit, its value of voltage is known and fixed. After the imputs are imposed, the electrical signal travels throught the network and the resulting output potential is read in the nodes belonging to \textit{output nodes}. The remaining nodes, that do not function as input or output, are included in the set of \textit{hidden nodes}.

The nodes in the PNN are connected by $\M$ memristive elements. Consequently, a memristor connecting two nodes experinces potential drop, pressure drop and concentration gradient given by the difference between the node where the base of the memristor is connected to, and the node to which the tip is linked. The memristor is characterized by its conductance state $g_{\m}$ with $\m\in[1,\M]$, consequence of both the geomtery parameters and the external stimuli.

To solve numerically the resulting circuit, it is necessary to simultaneously address the circuit equations and the time evolution equation for the memristors' conductance. 
The circuit equations are handled using the open-source circuit solver \textsc{AHKAB}\cite{}, which computes the electric potentials at the nodes and the currents through the edges discretizing the differential equations in timesteps $\Delta t$. To incorporate the memristive elements, we approximate the time evolution of the conductances using a discretized form of \cref{eq:eqofmotconductance}, employing the Euler-forward method. Specifically, the conductance at a given timestep is calculated from its value at the previous timestep as:
\begin{multline}
g_{\m}\brac{\boldsymbol{\Delta}(t), t} = g_{\m}\brac{\boldsymbol{\Delta}(t), t-\Delta t} + \\
 \frac{g_{\m,\infty}\brac{\boldsymbol{\Delta}(t)-g_{\m}\brac{\boldsymbol{\Delta}, t}}}{\tau}\Delta t.
\end{multline}
In this work, we only apply constant voltage supplies to the input nodes of the circuit. Nonetheless, we notice that solving the dynamical equations is needed due to the dynamical change in the conductance of the memristors that, in turn, results in time-dependent potential drops across them. A more detailed illustration of the algorithm that solves the circuit, along with an analysis of the relaxation to the steady state of the memristor network, is provided in \cref{app:dyn_evol_steady_state}.

\section{\label{sec:training}Training Algorithm for a Memristor Network}

We developed a training algorithm that makes use of the physical dependencies of memristor conductivity on both external stimuli and geometric properties to implement a physical neural network. These dependencies allow us to interpret such physical parameters as tunable weights, analogous to standard artificial neural networks, and to encode learning directly into the physical structure of the system. As in traditional ANNs, the goal of the training algorithm is to modify these weights so that the network maps a given set of inputs to a corresponding set of desired outputs. In the case of the memristor network considered here, training aims to produce specific voltage values at designated output nodes, given fixed voltage inputs at the input nodes.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\columnwidth]{plots/training/two_types_weights.pdf}
    \caption{(a) Memristor network with weights defined in the nodes. (b) Memristor network where wegiths characterize each edge.}
    \label{fig:two_types_weights}
\end{figure} 
Crucially, the physical quantities that influence memristor conductance fall into two distinct categories: external stimuli, such as pressure or concentration, that are applied to the nodes, and geometric parameters, such as channel length or base radius, that are defined on the edges. As a result, we distinguish between two types of training strategies: \emph{node-defined weight training}, as illustrated in \cref{fig:two_types_weights}(a), and \emph{edge-defined weight training}, as shown in \cref{fig:two_types_weights}(b).

A modification to any of these weights affects the network's behavior by altering the functional form of the memristor’s steady-state conductance, $g_{\infty}(\boldsymbol{\Delta})$, which depends on the local potential, pressure, and concentration drops across the device. Importantly, the \emph{entire functional shape} of $g_{\infty}$ plays a role in determining the network’s dynamics and ultimate steady state, as illustrated in \cref{app:dyn_evol_steady_state}.

In this work, we focus on training a single property as the weight, while keeping all other parameters fixed as initially defined. For example, we may select an external stimulus, such as the imposed pressure or the density at each node. Alternatively, we can choose the length or base radius of the channel. The training process is carried out by iterating a scheme, illustrated in \cref{fig:training_scheme}, which consists of two phases: a forward pass and a backward pass.
\begin{figure*}[t] 
    \centering
    \includegraphics[width=\textwidth]{plots/training/training_scheme.pdf}
    \caption{Scheme representing the two phases of the training algorithm, they are iteraitvely performed for a selected number of training steps.}
    \label{fig:training_scheme}
\end{figure*}
In the forward phase, the network is fed with specific electric potentials in the input nodes. Following both circuit laws and memristor's dynamics, the network reaches a steady state where electrical potentials can be measured in the output nodes. Clearly, the output of the network depends on the set of weights imposed on the network $\boldsymbol{w}$, regardless them being node or edge defined. The value of the outputs can be used to quantify the status of the network, meaning how much its answer differs from the desired one. Specifically, we define a cost function that computes the difference squared between the voltage in the output nodes and the values of desired voltages
\[
C\left( \boldsymbol{w} \right) = \sum_{i \in \Lambda} \brac{V_i\brac{\boldsymbol{w}} - V_i^D}^2,
\label{eq:cost_func}
\]
where $\Lambda$ represents the set of indeces belonging to the output nodes, $V_i\brac{\boldsymbol{w}}$ the voltage measured on the output nodes and $V^D$ is a vector containing the values of desired output nodes.

The backward phase has the fundamental purpose of finding a set of weights that minimize the cost function. We follow the method of the \textit{steepest descent}, for which the update of the weights from one training step $s$ to the following $s+1$,
\begin{equation}
\boldsymbol{w}^{s+1} = \boldsymbol{w}^{s} - \alpha \boldsymbol{\nabla}C\brac{\boldsymbol{w}},
\label{eq:weight_update}
\end{equation}
results in a decrease in the cost funtion until a minimum is reached, for sufficienlty small values of $\alpha$ \cite{steepest_descent_book}. The gradient in the update rule is computed using a \textit{finite differences method}
\begin{equation}
\boldsymbol{\nabla}C\brac{\boldsymbol{w}} = \frac{C\brac{\boldsymbol{w}+\boldsymbol{\Delta w}}-C\brac{\boldsymbol{w}}}{\Delta w},
\label{eq:gradient_cost_func}
\end{equation}
where $\boldsymbol{\Delta w}$ is a vector with the same number of entries as the weight vector, but every entry has a value $\Delta w$. 

In the training algortihm, we first initialize the system with the memristor parameters specified in \cref{sec:memristor_network}, an initial pressure of $1bar$ and concentration of $0.2mM$ at each node. With the knowledge of input and desired output voltage values, it is possible to performe the first forward pass, from which we obtain the cost function $C\brac{\boldsymbol{w}}$, that we store since it is needed in the computation its gradient. Subsequently, we proceed with the backward pass. We compute each entry of the gradient in \cref{eq:gradient_cost_func} by selecting the corresponding weight, a node or an edge parameter, increment its value of $\Delta w$ and applying a forward step to obtain the corresponding entry of $C\brac{\boldsymbol{w}+\boldsymbol{\Delta w}}$. With the stored value of $C\brac{\boldsymbol{w}}$, we are able to compute the gradient in \cref{eq:gradient_cost_func} and use it to calculate the weight update in \cref{eq:weight_update}. 

Althought performed numerically in this work, we highlight that the feedforward pass is a physical process in which the system natually relaxes to its steady state. Moreover, also the backward pass we use can be translated to a physical event since involves performing the forward pass a number of time as the number of the weights. Once the gradient of the cost function is determined and stored, only the simple algebric calculations of \cref{eq:weight_update} are left to be computed. We propose this method as a hybrid physical and in-silico training algorithm, as a future experimental approach to the probelm.

\subsection{Training a Memristor Voltage Divider}

A voltage divider is an electrical circuit made of three nodes connected in series by two resistances, that returns a fraction of the input voltage. Given the simplicity of its structure, we use it as an initial playground for the training algorithm described above. Specifically, we replace the resistances with memristor channels, to be trained such that giving $V_1 = 5V$ and $V_3 = 0V$ as inputs, we get $V_2 = 4V$ as output. In \cref{fig:vd_scheme} a schematic of the memristor voltage divider is shown.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{plots/voltage_divider/vd_scheme.pdf}
    \caption{}\label{fig:vd_scheme}
\end{figure} 

We initialize the system with pressures $P_1 =  P_2 = 1 bar$ and ion concetration $\rho_1 = \rho_2 = 0.2 mM$ on each node, length $L_1 = L_2 = 10\mu m$ and base radius  $R_{b1} = R_{b2} = 200 nm$ on each edge. After choosing one of these four physical quantities as training parameter $\boldsymbol{w} = \brac{w_1, w_2}$, we implement the training as descibed in \cref{sec:training} that modifies them, while the other parameters are kept constant. 
In \cref{fig:mse_weights_vd} we show that the cost function normalized to its initial value decreases of many order of magnitude during the training for each of the choosen weight. The values of the choosen set of weights simultaneouly changes from the initial condition, as shown in Figures [], adjusting the behaviour of the network to the desired one. If the values of a choosen type of weight at the end of the training are imposed to the netowork together with the voltage inputs, the network will autonomously realx to a staedy state that coincides with the desired state. To exemplify this point, the inset in Figure shows the physical relaxation of the voltage divider when the trained values of pressure, which correspond to the three values in the last training step, are imposed on the nodes together with the constant inputs voltages. The inset shows that the output voltage $V_2$ autonomously relaxes to the desired value of $V_D=4V$ in a time scale characteristic of the memristor.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{plots/voltage_divider/mse_weights_vd.pdf}
    \caption{(a) Behaviour of the cost function during the training of the voltage divider. Different colors indicate that the training is done with a specific choise of training weights, as indiacted in the legend. (b) Pressures imposed on the three nodes of the voltage divider during training. The inset shows the relaxation to the steady state of the output voltage $V_2$ to the desired value $V_D$ when the pressures imposed are the values obtained in the last training step, and the input nodes are stimulated with fixed voltage. (c) Ion concentration desnities imposed on the three nodes of the voltage divider during training. (d) Length of the memristors during training. (e) Base radius of the memristors during training.}\label{fig:mse_weights_vd}
\end{figure} 

In order to investigate the adaptability of the algorithm, we trained the system with a set of three consequent desired voltages $V_D=\{1V, 3V, 4V\}$. In \cref{fig:evolution_targets} we show how the output voltgage $V_2$ subsequently adapts to the desired voltages during the training of the pressures in the nodes, the evolution of which can be seen in \cref{fig:evolution_targets}. It is evident that the system can be retrained at any stage to a new desired objective making it reusable. Any other weight type can be chosen, depending on the necessity.  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{plots/voltage_divider/evolution_targets.pdf}
    \caption{(a) The trainiang of a memristor voltage divider to a set of desired voltage outputs $V_D = {1V, 3V, 4V}$, indicated by dashed light purple lines. The output voltage $V_2$, indicated by dark purple dots, adapts to the desired output during the training. The input voltages $V_1$ and $V_3$ are constant at $5V$ and $0V$ respectively. (b) The dots with different shades of gray show the values of pressures, chosen as weights, imposed on the three nodes during training.}\label{fig:evolution_targets}
\end{figure}

\subsection{Training a memristor network}

To generalize the training objective, we consider a memristor network constructed with a more complex geometry. \cref{fig:mse_general}(a) illustrates a network with three input nodes and two output nodes. The network is trained to produce output voltages of $[3,V, 4,V]$ when provided with input voltages of $[0,V, 2,V, 5,V]$.  Reaching a desired input-output voltage mapping in a generic electric circuit can be useful for variuous electronic purpuses \cite{voltage_controlled_oscillator, onchip_powerdistributed}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/general_network/mse_general.pdf}
    \caption{(a) Generic network with $9$ nodes and $15$ edges with inputs $[V_{2}, V_{4}, V_{9}] = [0V, 2V, 5V]$ and desired outputs $[V_{5}^D, V_{7}^D] = [3V, 4V]$, delta weights a third of their intial value and learning rates $[0.001, 1, 0.0001, 0.001]$. (b) Cost funtion during training of generic graph for input-output mapping task. The different colors indicate different weight choice.}\label{fig:mse_general}
\end{figure} 

The result of the training is presented in \cref{fig:mse_general}(b), which shows how the cost function responds to the wieght updates performed in the algorithm, for each of the four different weights. All of the possible different choices of weight result in a trained set that decreases the initial error of many orders of magnitudes. 

% \cleardoublepage
\subsection{Linear Regression}

We broaden the training capability of the network by including a linear regression task, where, in its simplest form, the network is trained to give potential output $V_{\text{out}}$ in a certain node of the network, related to the input potential $V_{\text{in}}$ via a linear relationship
\begin{equation}
    V_{\text{out}} = \text{a} V_{\text{in}} + \text{b}.
\label{eq:linear_relationship}
\end{equation}
We generate two sets of data, a training set $ \{ V_{\text{in}}^{(j)}, V_{\text{out}}^{(j)} \}_{j=1}^{N_{\text{train}}}$ and a test set $ \{ V_{\text{in}}^{(j)}, V_{\text{out}}^{(j)} \}_{j=1}^{N_{\text{test}}}$, where $N_{\text{train}}$ and $N_{\text{train}}$ denote the number of training and test samples, respectively. To create the training set, a sequence of input voltages is randomly sampled from the interval $[1V, 4V]$, and the corresponding output voltages are computed using equation \cref{eq:linear_relationship}, ensuring that the input-output pairs satisfy the desired relationship.
The test set is constructed in a similar way, but uses $N_{\text{test}}=20$ input values that are evenly spaced within the same interval. Their corresponding outputs are also computed using equation \cref{eq:linear_relationship}. Note that the training and test sets contain different input values.

The training algorithm described in \cref{sec:training} can be slightly adapted to support this training objective. At each training step, an input-output voltage pair is selected from the training set and used as the input voltage and target output in the algorithm. This differs from the original algorithm, where the same input-output pair was presented at every step.

Therefore, at each training step $ j \in[1, N_{\text{train}}] $, the cost function used to compute the gradient in \cref{eq:gradient_cost_func} is adapted to
\[
C_j(\boldsymbol{w}) = \left[ V_{\text{out}}(\boldsymbol{w}) - V_{\text{out}}^{(j)} \right]^2,
\]
where $ V_{\text{out}}(\boldsymbol{w}) $ is the measured output voltage for the selected input, and $ V_{\text{out}}^{(j)} $ is the corresponding target output in the training set. This expression corresponds to a simplified version of \cref{eq:cost_func} for the case of a single output node.

To evaluate how well the algorithm performs during training, we compute the \emph{test cost function} $ C_{\text{test}} $ over the entire test set. If $ \boldsymbol{w} $ denotes the weight vector at a given training step, the test cost is defined as
\[
C_{\text{test}}\left( \boldsymbol{w} \right) = \sum_{j=1}^{N_{\text{test}}} \left[ V_{\text{out}}(\boldsymbol{w}) - V_{\text{out}}^{(j)} \right]^2,
\]
which sums the squared differences between the measured and target output voltages at each test point. For each input $V_{\text{in}}^{(j)}$ in the test set, the voltage $ V_{\text{out}}(\boldsymbol{w}) $ is the output produced by the network with weights $ \boldsymbol{w} $, while $ V_{\text{out}}^{(j)} $ is the desired output. This test cost is used for performance evaluation and is not involved in the training process.

Since the task is more demanding than a single input-output mapping, we use a larger network with more degrees of freedom. \cref{fig:lin_regr1}(a) shows the graph selected for the regression task. It consists of 20 edges and 10 nodes, three of which are inputs and one is an output. Among the three input nodes, one is set to ground and another to a constant value, in order to enable the training of a linear function with an offset. For the training objective, we aim to train the network to fit the line $V_{\text{out}} = 0.2 V_{\text{in}} + 0.3$ over the input voltage range $[1V, 5V]$.

The performance of the algorithm on this task is shown in \cref{fig:lin_regr2}(b), where the learning rates for the different weights have been fine-tuned to prevent overshooting while still maintaining comparable convergence speed. From the behavior of the cost function, it is evident that the choice of weight type has a decisive impact on the network’s ability to learn a complex task. Specifically, training the lengths of the channles on the edges results in successful convergence, whereas training the node pressures proves to be extremely inefficient. For a visual interpretation of the regression cost function, \cref{fig:lin_regr1}(c) shows the state of the training at three different time steps.



\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/regression/G00010001.pdf}
    \caption{}\label{fig:lin_regr1}
\end{figure} 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/regression/G00010002.pdf}
    \caption{}\label{fig:lin_regr2}
\end{figure} 


To study the influence of memristor orientation on training performance, we retained the same network geometry but inverted the orientation of some memristors, as shown in \cref{fig:lin_regr2}(a). Specifically, if in the original configuration the base of a memristor pointed to one node and the tip to another, we reversed this direction.

This modification significantly affects the behavior of the network. As shown in \cref{fig:lin_regr2}(b), the new orientation alters the trainability of different weight types: in this configuration, training node concentration leads to better performance, while training edge length becomes less efficient. The underlying reason is that inverting the orientation of a channel also inverts the sign of the external stimulus applied to it, since the stimuli are defined node-wise and the difference is always taken as "base minus tip."

This reversal changes both the shape and the conductivity range over which the function $g_{\infty}(\boldsymbol{\Delta})$ is defined, thereby limiting or enabling the conductivity values the channel can attain. A network with well-oriented channels can therefore access conductivity ranges that make the training solvable, while a poorly oriented network may be constrained to regions where solutions are not found. To exemplify this point, Figure~(X) of Ref.~(Y) shows $g_{\infty}$ as a function of pressure for positive and negative potential drops, assuming a zero concentration drop. The plot reveals two completely disjoint conductivity ranges depending on the sign of the potential drop, which explains why training on pressure alone may be challenging. This highlights the need for deeper exploration of network geometry when selecting the type of weight to train. In this regard, we observe that the edges whose orientation was inverted are directly connected to either the input or the output nodes. This suggests that particular attention should be given to these edges in future explorations, as they may have a bigger impact on trainability.
As for the length-based training, \cref{fig:lin_regr2}(c) illustrates how the network responds to the test set at three different stages of the training process.

Bigger networks have more degrees of freedom, and should allow for a bigger range of exploration of a solution. However, the experimantal realization would become more challenging. In this work, we show that an adequate geometry for a specific training goal can be sufficent to make a small network trainable. A deper analysis on the influence of geometry on the trainabiliy is needed in the research contest of physical learning and can be explored in future research.

Larger networks offer more degrees of freedom and can, in principle, enable broader exploration of the solution space. However, their experimental realization becomes increasingly complex. In this work, we showed that an appropriately chosen geometry adapted to a specific training objective can be sufficient to make even a small networks trainable. A deeper analysis of the role of geometry in determining trainability is still needed in the context of physical learning.

\section{Conclusions}

We proposed a physical neural network composed of nodes connected by aqueous memristors. These devices exhibit conductance that responds to external stimuli of various physical natures, such as electrical, mechanical, and chemical. Additionally, their conductivity is sensitive to changes in the geometric parameters of their channel-like structure.

Our motivation for using these devices lies in the ability to exploit these dependencies to control the conductance of each memristor in the network. This enables us to manipulate an input electrical signal in a programmable way, according to a specific computational objective.

To this end, we developed a training algorithm that targets one of the physical parameters influencing conductivity and treats it as the analog of a weight in a standard machine learning framework. The electrical signal is used as both input and output, while other types of signals or geometric dependencies are used to adjust the network during training.

The training process consists of two phases. The first is an inference phase, in which input voltages are applied to the system and the circuit is allowed to physically relax to a steady state. At this point, the output voltages can be read. Ideally, these outputs should match a predefined set of target values reflecting the training goal. In the second phase, the update phase, the weights are adjusted to minimize a cost function based on the difference between the actual and desired outputs. By iteratively applying this procedure, the network is gradually tuned to achieve the desired input-output behavior.

The inference phase is entirely physical, as the system evolves naturally to its steady-state response when inputs are applied. In contrast, the update phase is only partially physical. The weight update relies on estimating the gradient of the cost function using a finite-differences method, following a steepest descent rule. Specifically, each weight is perturbed slightly, and the resulting change in the cost function is computed. The gradient is then approximated by taking the difference in cost between the perturbed and unperturbed configurations, divided by the magnitude of the perturbation. While an external computer is required to store and coordinate the gradients for each weight, the values themselves can be obtained through physical measurements of the system's output under the two conditions. This structure allows for a mostly physical implementation of learning, with digital support.

As a first simple application, we tested the algorithm on a voltage divider, training the network to produce a specific fraction of the input voltage at the output node. The training was successful across all four selected weight types: pressure drop, concentration gradient across the memristor, length of the memristor, and its base radius.After selecting a particular weight type, we further demonstrated that the network, once trained to achieve a specific objective, could be retrained to a different target with similar training speed. This result indicates that the algorithm is both adaptive to new training objectives and largely independent of the initial weight configuration.

Training larger and more complex memristor networks is also feasible with the algorithm we proposed. We demonstrated that a network composed of 9 nodes and 15 edges can be successfully trained for input-output voltage mapping using all the weight types considered.

We then challenged a more complex network, with 10 nodes and 20 edges, to perform a linear regression task with an offset. In this setting, we observed that the trainability of the network using a specific weight type strongly depends on the network's geometry. First, we showed that different geometries yield different final training accuracies when using the same weight type. Then, keeping the geometry fixed, we demonstrated that the orientation of the memristors also plays a crucial role: reversing the direction of just three memristors connected to either input or output nodes improved the training precision when using concentration as the trained weight by three orders of magnitude.

Given that the expressiveness of deep neural networks grows exponentially with depth\cite{big_neuralnet1, big_neuralnet2}, we interpret the limitations encountered in training linear regression tasks as a consequence of the small size of the network. However, small-scale networks are experimentally attractive, and in such cases, a training error of a few orders of magnitude may be acceptable. A key challenge for scaling such networks lies in the physical control of individual weights, especially when the weight type involves mechanical or geometric parameters. Emerging technologies in microfluidics, soft robotics, and programmable matter may provide practical tools to overcome these limitations and scale the network while preserving control.

The core intention behind the algorithm is to enable a physically realizable neural network that requires minimal computational resources to operate. Thanks to the multiple physical dependencies of memristors, such as electrical, chemical, mechanical and geometrical factors, the algorithm supports a wide range of possible weight types. This flexibility makes it highly adaptable to various experimental scenarios, depending on which parameters are most accessible or controllable in a given setup. [.. examples of realizable system, cite literature].

\cleardoublepage
\appendix

\section{Dynamical evolution to the steady state \label{app:dyn_evol_steady_state}}

In this section, we provide a more detailed explanation of how a memristor network relaxes to its steady state. Alongside outlining the algorithm used to solve the circuit, we highlight the importance of the dependence on the functional form of the steady-state conductance, which itself is determined by the values of the weights. Selecting appropriate weights shapes the  staedy-state conductance function in such a way that the network naturally relaxes to a steady state aligned with the training objective.

For simplicity, we consider the voltage divider shown in \cref{fig:app_vd}, where the memristors are labeled as $ \m_1 $, connecting node 2 to node 1, and $ \m_2 $, connecting node 3 to node 2. We also denote the corresponding potential drops across each memristor as $ \Delta V_1 = V_2 - V_1$ and $ \Delta V_2  = V_3 - V_2$, respectively. To simplify, we set both the pressure difference ($ \Delta P $) and concentration difference ($ \Delta \rho $) to zero. In this example, we choose the channel length as the training weight.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\columnwidth]{plots/appendixA/app_vd.pdf}
    \caption{Schematical representation of a memristor voltage divider. Two memristors, \( M_1 \) and \( M_2 \), connect two input nodes (green dots) with fixed voltages of \( 0\,\mathrm{V} \) and \( 5\,\mathrm{V} \), respectively, to a single output node (purple dot). Each memristor is subject to one external stimulus: the electric potential difference between the nodes it connects.
    }
    \label{fig:app_vd}
\end{figure} 
The dynamical evolution of the conductance of each memristor, discretized in time-steps of size $\Delta t$, reduces to the update rule:
\begin{equation}
g_{\m}(t + \Delta t) = g_{\m}(t) +
\frac{g_{\m,\infty}(\Delta V_{\m}(t)) - g_{\m}(t)}{\tau} \Delta t,
\label{eq:conduct}
\end{equation}
with initial condition $g_{\m}(t = 0) = g_0$. The potential drops across the memristors, $\Delta V_1(t)$ and $\Delta V_2(t)$, are computed from Kirchhoff's laws. In the case of a simple voltage divider ,see \cref{fig:app_vd}, these simplify to:
\begin{equation}
\Delta V_1(t) = \frac{5 g_2(t)}{g_1(t)+g_2(t)}, \qquad
\Delta V_2(t) = \frac{5 g_1(t)}{g_1(t)+g_2(t)},
\label{eq:potentials}
\end{equation}

The simulation proceeds as follows:

\begin{itemize}
\item At $t = 0$, initialize: $g_1(0) = g_2(0) = g_0$.
\item Compute $\Delta V_1(0)$ and $\Delta V_2(0)$ using \cref{eq:potentials}.
\item Use \cref{eq:conduct} to update $g_1(\Delta t)$ and $g_2(\Delta t)$.
\item Repeat the process at each time step until $|g_{\m}(t) - g_{\m,\infty}(t)| < \epsilon$ for all memristors.
\end{itemize}

\begin{algorithm}
    \caption{Relaxation of a Memristor Voltage Divider to Steady State}
    \begin{algorithmic}[1]
    \State \textbf{Input:} Initial conductance $g_0$, time step $\Delta t$, convergence threshold $\epsilon$, relaxation time $\tau$
    \State Initialize $g_1 \gets g_0$, $g_2 \gets g_0$, $t \gets 0$
    \While{not converged}
        \State Compute $\Delta V_1 \gets \frac{5 \cdot g_2}{g_1 + g_2}$
        \State Compute $\Delta V_2 \gets \frac{5 \cdot g_1}{g_1 + g_2}$
        \State $g_1 \gets g_1 + \frac{g_{\infty}(\Delta V_1) - g_1}{\tau} \cdot \Delta t$
        \State $g_2 \gets g_2 + \frac{g_{\infty}(\Delta V_2) - g_2}{\tau} \cdot \Delta t$
        \If{$|g_1 - g_{\infty}(\Delta V_1)| < \epsilon$ \textbf{and} $|g_2 - g_{\infty}(\Delta V_2)| < \epsilon$}
            \State \textbf{break}
        \EndIf
    \EndWhile
    \end{algorithmic}
\end{algorithm}
    

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{plots/appendixA/grid_initial_condition.pdf}
    \caption{Relaxation toward the steady
    state when $g_{1,\infty}(V)=g_{2,\infty}(V)$.}
    \label{fig:memristor_network}
\end{figure} 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{plots/appendixA/grid_trained.pdf}
    \caption{Relaxation toward the steady state when the system has been trained to give $V_2 = 4V$. The training sets the two different functional forms of $g_{\infty}$ seen in the right figure.}
    \label{fig:memristor_network}
\end{figure} 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\columnwidth]{plots/appendixA/grid_trained_change_func.pdf}
    \caption{We consider the trained system in Figure 3, but we define the funtion $g_{1, \infty}$ such that it passes througth the steady state point (triangle in Figure 3), now indicated with a red cross, but with a different functional form. Due to the different functinal form, the system reaches a steady state that doesn't coincide with the desired in Figure 3.}
    \label{fig:memristor_network}
\end{figure} 



\clearpage
\bibliographystyle{unsrt} 
\bibliography{biblio}

\end{document}
